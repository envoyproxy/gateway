dashboard:
  labels: {}

# Values for Grafana dependency
grafana:
  enabled: true
  fullnameOverride: grafana
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          url: http://prometheus
  adminPassword: admin
  testFramework:
    enabled: false
  service:
    type: LoadBalancer
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: 'envoy-gateway'
          orgId: 1
          folder: 'envoy-gateway'
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards/envoy-gateway
  dashboardsConfigMaps:
    envoy-gateway: "grafana-dashboards"


# Values for Prometheus dependency
prometheus:
  enabled: true
  # To simplify the deployment, disable non-essential components
  alertmanager:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  server:
    fullnameOverride: prometheus
    persistentVolume:
      enabled: false
    readinessProbeInitialDelay: 0
    global:
      # Speed up scraping a bit from the default
      scrape_interval: 15s
    service:
      # use LoadBalancer to expose prometheus
      type: LoadBalancer
    # use dockerhub
    image:
      repository: prom/prometheus
    securityContext: {}


# Values for Fluent-bit dependency
# TODO: remove fluent-bit dependency
fluent-bit:
  enabled: true
  image:
    repository: fluent/fluent-bit # use image from dockerhub
  fullnameOverride: fluent-bit
  testFramework:
    enabled: false
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "2020"
    prometheus.io/path: /api/v1/metrics/prometheus
    fluentbit.io/exclude: "true"
  ## https://docs.fluentbit.io/manual/administration/configuring-fluent-bit/classic-mode/configuration-file
  config:
    service: |
      [SERVICE]
          Daemon Off
          Flush {{ .Values.flush }}
          Log_Level {{ .Values.logLevel }}
          Parsers_File parsers.conf
          Parsers_File custom_parsers.conf
          HTTP_Server On
          HTTP_Listen 0.0.0.0
          HTTP_Port {{ .Values.metricsPort }}
          Health_Check On

    ## https://docs.fluentbit.io/manual/pipeline/inputs
    inputs: |
      [INPUT]
          Name tail
          Path /var/log/containers/*.log
          multiline.parser docker, cri
          Tag kube.*
          Mem_Buf_Limit 5MB
          Skip_Long_Lines On

    ## https://docs.fluentbit.io/manual/pipeline/filters
    filters: |
      [FILTER]
          Name kubernetes
          Match kube.*
          Merge_Log On
          Keep_Log Off
          K8S-Logging.Parser On
          K8S-Logging.Exclude On
  
      [FILTER]
          Name grep
          Match kube.*
          Regex $kubernetes['container_name'] ^envoy$
  
      [FILTER]
          Name parser
          Match kube.*
          Key_Name log
          Parser envoy
          Reserve_Data True

    ## https://docs.fluentbit.io/manual/pipeline/outputs
    outputs: |
      [OUTPUT]
          Name                   loki
          Match                  kube.*
          Host                   loki.monitoring.svc.cluster.local
          Port                   3100
          Labels                 job=fluentbit, app=$kubernetes['labels']['app'], k8s_namespace_name=$kubernetes['namespace_name'], k8s_pod_name=$kubernetes['pod_name'], k8s_container_name=$kubernetes['container_name']


# Values for Loki dependency
loki:
  enabled: true
  # Running a single replica of Loki
  ## https://grafana.com/docs/loki/latest/setup/install/helm/install-monolithic/
  deploymentMode: SingleBinary
  fullnameOverride: loki
  loki:
    auth_enabled: false
    compactorAddress: "loki"
    memberlist: "loki-memberlist"
    commonConfig:
      replication_factor: 1
    storage:
      type: 'filesystem'
    rulerConfig:
      storage:
        type: "local"
  test:
    enabled: false
  singleBinary:
    replicas: 1
  read:
    replicas: 0
  backend:
    replicas: 0
  write:
    replicas: 0
  monitoring:
    lokiCanary:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
  # Disable gateway.
  gateway:
    enabled: false

# Values for Alloy dependency
alloy:
  enabled: false
  fullnameOverride: alloy
  alloy:
    configMap:
      content: |-
        // Write your Alloy config here:
        logging {
          level = "info"
          format = "logfmt"
        }
        loki.write "alloy" {
          endpoint {
            url = "http://loki.monitoring.svc:3100/loki/api/v1/push"
          }
        }
        // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
        // It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
        discovery.kubernetes "pod" {
          role = "pod"
        }
  
        // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
        // If no rules are defined, then the input targets are exported as-is.
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pod.targets
  
          // Label creation - "namespace" field from "__meta_kubernetes_namespace"
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action = "replace"
            target_label = "namespace"
          }
  
          // Label creation - "pod" field from "__meta_kubernetes_pod_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action = "replace"
            target_label = "pod"
          }
  
          // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "container"
          }
  
          // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action = "replace"
            target_label = "app"
          }
  
          // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "job"
            separator = "/"
            replacement = "$1"
          }
  
          // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "__path__"
            separator = "/"
            replacement = "/var/log/pods/*$1/*.log"
          }
  
          // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action = "replace"
            target_label = "container_runtime"
            regex = "^(\\S+):\\/\\/.+$"
            replacement = "$1"
          }
        }
  
        // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
        loki.source.kubernetes "pod_logs" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }
        // loki.process receives log entries from other Loki components, applies one or more processing stages,
        // and forwards the results to the list of receivers in the componentâ€™s arguments.
        loki.process "pod_logs" {
          stage.static_labels {
              values = {
                cluster = "envoy-gateway",
              }
          }

          forward_to = [loki.write.alloy.receiver]
        }
        

# Values for Tempo dependency
tempo:
  enabled: true
  fullnameOverride: tempo
  service:
    type: LoadBalancer


# Values for OpenTelemetry-Collector dependency
opentelemetry-collector:
  enabled: false
  fullnameOverride: otel-collector
  mode: deployment
  image:
    repository: "otel/opentelemetry-collector-contrib"
  config:
    exporters:
      prometheus:
        endpoint: "[${env:MY_POD_IP}]:19001"
      debug:
        verbosity: detailed
      loki:
        endpoint: "http://loki.monitoring.svc:3100/loki/api/v1/push"
      otlp:
        endpoint: tempo.monitoring.svc:4317
        tls:
          insecure: true
    extensions:
      health_check:
        endpoint: "[${env:MY_POD_IP}]:13133"
    processors:
      attributes:
        actions:
          - action: insert
            key: loki.attribute.labels
            # k8s.pod.name is OpenTelemetry format for Kubernetes Pod name,
            # Loki will convert this to k8s_pod_name label.
            value: k8s.pod.name, k8s.namespace.name
    receivers:
      jaeger:
        protocols:
          grpc:
            endpoint: "[${env:MY_POD_IP}]:14250"
          thrift_http:
            endpoint: "[${env:MY_POD_IP}]:14268"
          thrift_compact:
            endpoint: "[${env:MY_POD_IP}]:6831"
      datadog:
        endpoint: "[${env:MY_POD_IP}]:8126"
      zipkin:
        endpoint: "[${env:MY_POD_IP}]:9411"
      otlp:
        protocols:
          grpc:
            endpoint: "[${env:MY_POD_IP}]:4317"
          http:
            endpoint: "[${env:MY_POD_IP}]:4318"
      prometheus:
        config:
          scrape_configs:
          - job_name: opentelemetry-collector
            scrape_interval: 10s
            static_configs:
            - targets:
              - "[${env:MY_POD_IP}]:8888"
    service:
      telemetry:
        metrics:
          address: "[${env:MY_POD_IP}]:8888"
      extensions:
        - health_check
      pipelines:
        metrics:
          exporters:
            - prometheus
          receivers:
            - datadog
            - otlp
        logs:
          exporters:
            - loki
          processors:
            - attributes
          receivers:
            - otlp
        traces:
          exporters:
            - otlp
          receivers:
            - datadog
            - otlp
            - zipkin
